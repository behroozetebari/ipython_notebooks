{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the 20 best SA models for a basin, explore the variability in melt data that we generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%pylab notebook\n",
    "# import datetime as dt\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "#from nose.tools import set_trace\n",
    "from charistools.hypsometry import Hypsometry\n",
    "from charistools.meltModels import CalibrationCost\n",
    "from charistools.modelEnv import ModelEnv\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd ~/projects/CHARIS/derived_hypsometries/REECv0_ModelRank000/AM_Vakhsh_at_Komsomolabad\n",
    "#%ls *snow_on_land*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ddfs_and_surface(hyps):\n",
    "    '''\n",
    "    Gets the min/max ddfs from the hyps.comments\n",
    "    Returns a tuple with min, max\n",
    "    '''\n",
    "    min_ddf = np.nan\n",
    "    max_ddf = np.nan\n",
    "    surface = \"unknown\"\n",
    "    pmin = re.compile(r\"min_DDF_mm_pday_pdegC\") \n",
    "    pmax = re.compile(r\"max_DDF_mm_pday_pdegC\")  \n",
    "    psurface = re.compile(r\"melt for\")\n",
    "    for line in hyps.comments:\n",
    "        if pmin.search(line):\n",
    "            key, value = line.split(\" : \")\n",
    "            min_ddf = float(value)\n",
    "        if pmax.search(line):\n",
    "            key, value = line.split(\" : \")\n",
    "            max_ddf = float(value)\n",
    "        if psurface.search(line):\n",
    "            surface, dummy = psurface.split(line)\n",
    "            surface = surface[1:].lstrip().rstrip()\n",
    "            surface = surface.replace(\" \", \"_\")\n",
    "\n",
    "    return (min_ddf, max_ddf, surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drainageIDs = [\"IN_Hunza_at_DainyorBridge\", \n",
    "               \"AM_Vakhsh_at_Komsomolabad\", \n",
    "               \"SY_Naryn_at_NarynTown\", \n",
    "               \"GA_SaptaKosi_at_Chatara\",\n",
    "               \"GA_Karnali_at_Benighat\"]\n",
    "\n",
    "dir = \"/Users/brodzik/projects/CHARIS/derived_hypsometries\"\n",
    "for drainageID in drainageIDs:\n",
    "    df = pd.DataFrame([])\n",
    "    #for cycle in np.arange(20) + 81:\n",
    "    for rank in np.arange(20):\n",
    "        for year in np.arange(14) + 2001:\n",
    "        \n",
    "            # Get a list of melt filenames for this drainageID, cycle (or rank) and year\n",
    "            # There will be three, one for each surface type\n",
    "            #list = sort(glob.glob(\"%s/REECv0_Cycle%03d/%s/*%d*\" % (dir, cycle, drainageID, year)))\n",
    "            #print(cycle, year)\n",
    "            list = sort(glob.glob(\"%s/REECv0_ModelRank%03d/%s/*%d*\" % (dir, rank, drainageID, year)))\n",
    "            print(rank, year)\n",
    "            print(list)\n",
    "            #row = {\"cycle\":cycle,\n",
    "            #       \"drainageID\":drainageID}\n",
    "            row = {\"rank\":rank,\n",
    "                   \"drainageID\":drainageID}\n",
    "            for f in list:\n",
    "                hyps = Hypsometry(f)\n",
    "                total_melt_km3 = hyps.data.sum().sum()\n",
    "                min_ddf, max_ddf, surface = get_ddfs_and_surface(hyps)\n",
    "                row[\"year\"] = hyps.data.index[0].year\n",
    "                row[\"%s_min_ddf\" % (surface)] = min_ddf\n",
    "                row[\"%s_max_ddf\" % (surface)] = max_ddf\n",
    "                row[\"%s_melt_km3\" % (surface)] = total_melt_km3\n",
    "            \n",
    "            result = pd.DataFrame.from_dict(row, orient='index').transpose()\n",
    "        \n",
    "            #result = result[[\"year\", \"cycle\", \"drainageID\", \n",
    "            #                 \"Snow_on_land_min_ddf\", \"Snow_on_land_max_ddf\", \"Snow_on_land_melt_km3\",\n",
    "            #                 \"Snow_on_ice_min_ddf\", \"Snow_on_ice_max_ddf\", \"Snow_on_ice_melt_km3\",\n",
    "            #                 \"Exposed_glacier_ice_min_ddf\", \"Exposed_glacier_ice_max_ddf\", \"Exposed_glacier_ice_melt_km3\"]]\n",
    "\n",
    "            result = result[[\"year\", \"rank\", \"drainageID\", \n",
    "                             \"Snow_on_land_min_ddf\", \"Snow_on_land_max_ddf\", \"Snow_on_land_melt_km3\",\n",
    "                             \"Snow_on_ice_min_ddf\", \"Snow_on_ice_max_ddf\", \"Snow_on_ice_melt_km3\",\n",
    "                             \"Exposed_glacier_ice_min_ddf\", \"Exposed_glacier_ice_max_ddf\", \"Exposed_glacier_ice_melt_km3\"]]\n",
    "        \n",
    "            df = df.append(result)\n",
    "    \n",
    "    #outfile = \"%s/REECv0_CycleSummary/%s.annual_melt.last20.dat\" % (dir, drainageID)\n",
    "    outfile = \"%s/REECv0_ModelRankSummary/%s.annual_melt.best20.dat\" % (dir, drainageID)\n",
    "    print(\"Output to: %s\" % outfile, file=sys.stderr)\n",
    "    df.to_pickle(outfile)\n",
    "                         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for drainageID in drainageIDs:\n",
    "    #file = \"%s/REECv0_CycleSummary/%s.annual_melt.last20.dat\" % (dir, drainageID)\n",
    "    #print(\"last20 file %s\" % file, file=sys.stderr)\n",
    "    file = \"%s/REECv0_ModelRankSummary/%s.annual_melt.best20.dat\" % (dir, drainageID)\n",
    "    print(\"best20 file %s\" % file, file=sys.stderr)\n",
    "    df = pd.read_pickle(file)\n",
    "\n",
    "    melt = df.copy()\n",
    "    melt.drop(['Snow_on_land_min_ddf','Snow_on_land_max_ddf',\n",
    "               'Snow_on_ice_min_ddf','Snow_on_ice_max_ddf',\n",
    "               'Exposed_glacier_ice_min_ddf','Exposed_glacier_ice_max_ddf'], axis=1, inplace=True)\n",
    "\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(8,12))\n",
    "\n",
    "    melt.boxplot(ax=axes[0],\n",
    "                 column='Snow_on_ice_melt_km3',\n",
    "                 by='year')\n",
    "    axes[0].set_title(\"Melt from Snow on Ice\")\n",
    "\n",
    "    melt.boxplot(ax=axes[1],\n",
    "                 column='Exposed_glacier_ice_melt_km3', \n",
    "                 by='year')\n",
    "    axes[1].set_title(\"Melt from Exposed Glacier Ice\")\n",
    "\n",
    "    melt.boxplot(ax=axes[2],\n",
    "                 column='Snow_on_land_melt_km3',\n",
    "                 by='year')\n",
    "    axes[2].set_title(\"Melt from Snow on Land\")\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_ylim([0., \n",
    "                     1.1 * melt[['Snow_on_land_melt_km3', 'Snow_on_ice_melt_km3', 'Exposed_glacier_ice_melt_km3']].max().max()])\n",
    "        ax.set_ylabel('Melt ($km^3$)')\n",
    "\n",
    "    fig.suptitle(\"Best 20 Models, %s\" % drainageID)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.94)\n",
    "    outfile = \"%s.png\" % file\n",
    "    plt.savefig(outfile)\n",
    "    print(\"Saving plot to %s\" % outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a plot of overal variability by basin and surface type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"/Users/brodzik/projects/CHARIS/derived_hypsometries\"\n",
    "drainageIDs = [\"IN_Hunza_at_DainyorBridge\", \n",
    "               \"AM_Vakhsh_at_Komsomolabad\", \n",
    "               \"SY_Naryn_at_NarynTown\", \n",
    "               \"GA_SaptaKosi_at_Chatara\",\n",
    "               \"GA_Karnali_at_Benighat\"]alldf = pd.DataFrame([])\n",
    "\n",
    "for drainageID in drainageIDs:\n",
    "    file = \"%s/REECv0_CycleSummary/%s.annual_melt.last20.dat\" % (dir, drainageID)\n",
    "    print(\"last20 file %s\" % file, file=sys.stderr)\n",
    "    df = pd.read_pickle(file)\n",
    "\n",
    "    melt = df.copy()\n",
    "    melt.drop(['Snow_on_land_min_ddf','Snow_on_land_max_ddf',\n",
    "               'Snow_on_ice_min_ddf','Snow_on_ice_max_ddf',\n",
    "               'Exposed_glacier_ice_min_ddf','Exposed_glacier_ice_max_ddf'], axis=1, inplace=True)\n",
    "    alldf = alldf.append(melt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alldf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10,5))\n",
    "\n",
    "alldf.boxplot(ax=ax,\n",
    "              column=['Snow_on_ice_melt_km3','Snow_on_land_melt_km3','Exposed_glacier_ice_melt_km3'],\n",
    "              by='drainageID')\n",
    "ax.set_title(\"Melt Title\")\n",
    "\n",
    "    #ax.set_ylim([0., \n",
    "    #                 1.1 * melt[['Snow_on_land_melt_km3', 'Snow_on_ice_melt_km3', 'Exposed_glacier_ice_melt_km3']].max().max()])\n",
    "ax.set_ylabel('Melt ($km^3$)')\n",
    "\n",
    "fig.suptitle(\"Best Models (2001-2014), last 20 cycles\")\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf = surf.replace(\" \", \"_\")\n",
    "surf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ddf_variation(drainageID, nstrikes=3):\n",
    "    \n",
    "    # Read SA summary file, this is DDFs and z at end of each cycle\n",
    "    dir = \"/work/charis/ti_model/calibrations_correct_cost\"\n",
    "    params = \"DDFnbr=10mm_N100_M050\"\n",
    "    list = glob.glob(\"%s/%s.%dstr_%s.SA_summary.dat\" % (\n",
    "        dir, drainageID, nstrikes, params))\n",
    "    if 1 != len(list):\n",
    "        print(\"Error looking for SA_summary file for %s\" % drainageID, file=sys.stderr)\n",
    "    SAFile = list[0]\n",
    "    print(\"SA_summary file : %s\" % SAFile, file=sys.stderr)\n",
    "    SAdf = pd.read_pickle(SAFile)\n",
    "    \n",
    "    # Drop first 61 rows of SA output, to limit analysis to stable stuff at end\n",
    "    num_cycles_to_drop = 61\n",
    "    stabledf = SAdf.drop(SAdf.index[np.arange(num_cycles_to_drop)])\n",
    "    summarydf = stabledf.describe()\n",
    "    \n",
    "    # Read all calibration stats output and parse best model parameters from it\n",
    "    list = glob.glob(\"%s/%s.%dstr_%s.z*Best*stats.png\" % (\n",
    "        dir, drainageID, nstrikes, params))\n",
    "    if 1 != len(list):\n",
    "        print(\"Error looking for stats file for %s\" % drainageID, file=sys.stderr)\n",
    "    statsFile = list[0]\n",
    "    print(\"stats file : %s\" % statsFile, file=sys.stderr)\n",
    "    \n",
    "    best = np.zeros(4)\n",
    "    best_low = np.zeros(4)\n",
    "    best_high = np.zeros(4)\n",
    "    \n",
    "    # Parse the best model ddfs from the filename\n",
    "    p = re.compile(r'Best(\\d+\\.\\d+)_(\\d+\\.\\d+)_(\\d+\\.\\d+)_(\\d+\\.\\d+)')\n",
    "    m = p.search(statsFile)\n",
    "    for i in np.arange(4):\n",
    "        best[i] = float(m.group(i+1))\n",
    "    \n",
    "    # Use the variation from the stable cycles to calculate best_minus1std and best_plus1std\n",
    "    best_low = best.copy()\n",
    "    best_high = best.copy()\n",
    "    \n",
    "    ddf = ['winter_snow_ddf', 'summer_snow_ddf', 'winter_ice_ddf', 'summer_ice_ddf']\n",
    "    for i in np.arange(len(ddf)):\n",
    "        best_low[i] = best_low[i] - summarydf.loc['std', ddf[i]]\n",
    "        best_high[i] = best_high[i] + summarydf.loc['std', ddf[i]]\n",
    "        \n",
    "    # Do QC to ensure that each range enforces low <= high\n",
    "    if best_low[0] > best_low[1]:\n",
    "        print(\"Warning: QC problem on low snow DDFs, forcing them to lower value\")\n",
    "        best_low[0] = best_low[1]\n",
    "    if best_low[2] > best_low[3]:\n",
    "        print(\"Warning: QC problem on low ice DDFs, forcing them to lower value\")\n",
    "        best_low[2] = best_low[3]\n",
    "        \n",
    "    if best_high[0] > best_high[1]:\n",
    "        print(\"Warning: QC problem on high snow DDFs, forcing them to higher value\")\n",
    "        best_high[1] = best_high[0]\n",
    "    if best_high[2] > best_high[3]:\n",
    "        print(\"Warning: QC problem on high ice DDFs, forcing them to higher value\")\n",
    "        best_high[3] = best_high[2]\n",
    "    \n",
    "    # Make model strings and return them\n",
    "    best_str = \"_\".join([\"%.2f\" % i for i in best])\n",
    "    best_low_str = \"_\".join([\"%.2f\" % i for i in best_low])\n",
    "    best_high_str = \"_\".join([\"%.2f\" % i for i in best_high])\n",
    "    \n",
    "    result = {'drainageID': drainageID,\n",
    "              'nstrikes': nstrikes,\n",
    "              'Best': best_str, \n",
    "              'High': best_high_str,\n",
    "              'Low': best_low_str}\n",
    "    \n",
    "    result = pd.DataFrame.from_dict(result, orient='index').transpose()\n",
    "    result.set_index('drainageID', inplace=True)\n",
    "    result = result[['nstrikes', 'Low', 'Best', 'High']]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drainageID = \"GA_Karnali_at_Benighat\"\n",
    "result = find_ddf_variation(drainageID, nstrikes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drainageIDs = ['SY_Naryn_at_NarynTown',\n",
    "               'AM_Vakhsh_at_Komsomolabad',\n",
    "               'IN_Hunza_at_DainyorBridge',\n",
    "               'GA_Karnali_at_Benighat',\n",
    "               'GA_Narayani_at_Devghat',\n",
    "               'GA_SaptaKosi_at_Chatara']\n",
    "strikes = [2, 3]\n",
    "df = pd.DataFrame([])\n",
    "for drainageID in drainageIDs:\n",
    "    for strike in strikes:\n",
    "        result = find_ddf_variation(drainageID, nstrikes=strike)\n",
    "        df = df.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drainageID = \"GA_Karnali_at_Benighat\"\n",
    "#nstrikes = 2\n",
    "def best_models(drainageID, nstrikes=3):\n",
    "    # Read SA summary file, this is DDFs and z at end of each cycle\n",
    "    dir = \"/work/charis/ti_model/calibrations_correct_cost\"\n",
    "    params = \"DDFnbr=10mm_N100_M050\"\n",
    "    list = glob.glob(\"%s/%s.%dstr_%s.SA_summary.dat\" % (\n",
    "        dir, drainageID, nstrikes, params))\n",
    "    if 1 != len(list):\n",
    "        print(\"Error looking for SA_summary file for %s\" % drainageID, file=sys.stderr)\n",
    "    SAFile = list[0]\n",
    "    print(\"SA_summary file : %s\" % SAFile, file=sys.stderr)\n",
    "    df = pd.read_pickle(SAFile)\n",
    "    \n",
    "    df.loc[:, \"model\"] = (\n",
    "        df[\"winter_snow_ddf\"].map(str) + \"_\" +\n",
    "        df[\"summer_snow_ddf\"].map(str) + \"_\" +\n",
    "        df[\"winter_ice_ddf\"].map(str) + \"_\" +\n",
    "        df[\"summer_ice_ddf\"].map(str))\n",
    "\n",
    "    df[\"nstrikes\"] = nstrikes\n",
    "\n",
    "    outfile = \"%s/%s.%dstr_%s.SA_summary.best20.dat\" % (\n",
    "        dir, drainageID, nstrikes, params)\n",
    "\n",
    "    df.to_pickle(outfile)\n",
    "    print(\"outfile: %s\" % outfile, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drainageIDs = ['SY_Naryn_at_NarynTown',\n",
    "               'AM_Vakhsh_at_Komsomolabad',\n",
    "               'IN_Hunza_at_DainyorBridge',\n",
    "               'GA_Karnali_at_Benighat',\n",
    "               'GA_Narayani_at_Devghat',\n",
    "               'GA_SaptaKosi_at_Chatara']\n",
    "strikes = [2, 3]\n",
    "for drainageID in drainageIDs:\n",
    "    for strike in strikes:\n",
    "        best_models(drainageID, nstrikes=strike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newfile = '/work/charis/ti_model/calibrations_correct_cost/IN_Hunza_at_DainyorBridge.2str_DDFnbr=10mm_N100_M050.SA_summary.best20.dat'\n",
    "newfile = '/work/charis/ti_model/calibrations_correct_cost/AM_Vakhsh_at_Komsomolabad.2str_DDFnbr=10mm_N100_M050.SA_summary.best20.dat'\n",
    "new = pd.read_pickle(newfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get the best model for cycle 95:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.at[82, \"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
