{
 "metadata": {
  "name": "",
  "signature": "sha256:145ada88a75f051f3ffdceedee543647e29238c2f59a3f849b63705367a93771"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "EASE Grid 2.0 via Python, pyproj, and gdal_translate"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Prelude"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This page is necessary because a snow data product I was trying to use is not georeferencing in standard off the shelf tools:\n",
      "\n",
      "    [fedora@lugosi ~]$ gdalinfo NETCDF:nhtsw100e2_20030107_20030113.nc:merged_snow_cover_extent\n",
      "    Warning 1: dimension #2 (cols) is not a Longitude/X dimension.\n",
      "    Warning 1: dimension #1 (rows) is not a Latitude/Y dimension.\n",
      "    Driver: netCDF/Network Common Data Format\n",
      "    Files: nhtsw100e2_20030107_20030113.nc\n",
      "    Size is 180, 180\n",
      "    Coordinate System is `'\n",
      "\n",
      "Note that the coordinate system is empty and there are a couple of warnings. Turns out, the same thing fixes both problems. Long story short, the file is missing coordinate variables for the rows and cols dimensions. Now it seems obvious that this is what the warning messages were trying to tell me, but at the outset I was too dense to pick up on it. \n",
      "\n",
      "To experience the entire thread of discovery, and/or learn how to mess around with netCDF using Python, read on. If you just want to see how to fix it, skip ahead to the section \"So why isn't it working?\""
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exploring the NetCDF file"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This section starts with the basics. Open the file, sniff around to see what's in there, and print out some key bits of information. The very first thing to do is to tell Python what libraries we're going to need. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import mpl_toolkits.basemap.pyproj as pyproj\n",
      "#import pyproj\n",
      "import netCDF4 as nc\n",
      "import os\n",
      "import numpy as np\n",
      "from mpl_toolkits.basemap import Basemap"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before opening this notebook, I grabbed an example data file from the National Snow and Ice Data Center. (ftp://sidads.colorado.edu/pub/DATASETS/nsidc0531_MEASURES_nhsnow_weekly100/2003/nhtsw100e2_20030107_20030113.nc) This file is supposed to be in what's known as EASE Grid 2.0 format. Unfortunately, no known GIS tools (commercial or open source) read the file. Our objective here is to come up with a method of converting the file to something that the tools can read.\n",
      "\n",
      "So the very first step is to change to the directory containing the file using os.chdir. It works much as you should expect. Give it the full path to where you want to be."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.chdir(\"/home/vagrant/ipynb/\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now it's time to open the file. When we imported the NetCDF library above, we told python to associate it with the abbreviation \"nc\". To open the file, we need to use a function called \"Dataset\" in the netcdf library. All we need to do is tell it what filename we're interested in. This looks like the following:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = nc.Dataset('nhtsw100e2_20030107_20030113.nc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have a variable \"d\" that we can use to access the contents of the netCDF file. The NetCDF library is pretty smart. It doesn't try to load the whole file into memory at this point, as many NetCDF files can be pretty large. You can use this \"d\" variable to look at the descriptive metadata, get a sense for the size of the grids, or eventually, ask for the actual data values. For the moment, let's just do the blindingly simple thing and \"print\" the variable to the terminal."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print d\n",
      "print d.variables['rows'][::]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[-8950004.        -8850004.        -8750004.        -8650004.        -8550004.\n",
        " -8450004.        -8350003.5       -8250003.5       -8150003.5       -8050003.5\n",
        " -7950003.5       -7850003.5       -7750003.5       -7650003.5       -7550003.5\n",
        " -7450003.5       -7350003.        -7250003.        -7150003.        -7050003.\n",
        " -6950003.        -6850003.        -6750003.        -6650003.        -6550003.\n",
        " -6450003.        -6350003.        -6250003.        -6150003.        -6050002.5\n",
        " -5950002.5       -5850002.5       -5750002.5       -5650002.5       -5550002.5\n",
        " -5450002.5       -5350002.5       -5250002.5       -5150002.5       -5050002.5\n",
        " -4950002.5       -4850002.        -4750002.        -4650002.        -4550002.\n",
        " -4450002.        -4350002.        -4250002.        -4150002.        -4050002.\n",
        " -3950001.75      -3850001.75      -3750001.75      -3650001.75      -3550001.75\n",
        " -3450001.75      -3350001.5       -3250001.5       -3150001.5       -3050001.5\n",
        " -2950001.5       -2850001.5       -2750001.25      -2650001.25      -2550001.25\n",
        " -2450001.25      -2350001.25      -2250001.25      -2150001.        -2050001.125\n",
        " -1950001.        -1850001.        -1750001.        -1650000.875\n",
        " -1550000.875     -1450000.875     -1350000.75      -1250000.75      -1150000.75\n",
        " -1050000.625      -950000.625      -850000.5625     -750000.5625\n",
        "  -650000.5        -550000.4375     -450000.40625    -350000.375\n",
        "  -250000.328125   -150000.28125     -50000.2421875    49999.796875\n",
        "   149999.84375     249999.875       349999.90625     449999.96875\n",
        "   550000.          650000.0625      750000.0625      850000.125\n",
        "   950000.1875     1050000.25       1150000.25       1250000.25\n",
        "  1350000.375      1450000.375      1550000.375      1650000.5        1750000.5\n",
        "  1850000.5        1950000.625      2050000.625      2150000.75       2250000.75\n",
        "  2350000.75       2450000.75       2550000.75       2650000.75       2750001.\n",
        "  2850001.         2950001.         3050001.         3150001.         3250001.\n",
        "  3350001.25       3450001.25       3550001.25       3650001.25       3750001.25\n",
        "  3850001.25       3950001.5        4050001.5        4150001.5        4250001.5\n",
        "  4350001.5        4450001.5        4550001.5        4650001.5        4750001.5\n",
        "  4850002.         4950002.         5050002.         5150002.         5250002.\n",
        "  5350002.         5450002.         5550002.         5650002.         5750002.\n",
        "  5850002.         5950002.         6050002.5        6150002.5        6250002.5\n",
        "  6350002.5        6450002.5        6550002.5        6650002.5        6750002.5\n",
        "  6850002.5        6950002.5        7050002.5        7150002.5        7250003.\n",
        "  7350003.         7450003.         7550003.         7650003.         7750003.\n",
        "  7850003.         7950003.         8050003.         8150003.         8250003.\n",
        "  8350003.         8450003.         8550003.         8650003.         8750003.\n",
        "  8850003.         8950003.       ]\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This simple command gives us a lot of information. This particular file has a lot of metadata, which is all printed out. Some files will have more and some will have less. For the moment, just notice that the \"Conventions\" attribute specifies \"CF-1.6\". That means that the rest of the attributes should be interpreted using the CF v1.6 specification found on http://cfconventions.org/. The real meat and potatoes of a NetCDF file are the \"dimensions\" and \"variables\". All useful NetCDF files will have dimensions and variables.\n",
      "\n",
      "Dimensions have a name and a size. We can see from the above that this file has three dimensions (time, rows, cols). The time dimension has a size of 1, while rows and cols both have a size of 180. Dimensions in and of themselves are not very interesting. They exist mainly to specify the shape of gridded data stored in Variables.\n",
      "\n",
      "Variables have a name, a data type, and zero or more dimensions. If a variable has zero dimensions (like \"coord_system\", above), it is a scalar and can only store one value. Otherwise, the dimensions specify the shape of the grid. The time variable has one dimension, also named time. Latitude and longitude have two dimensions (rows and cols), and everything else except \"coord_system\" has three dimensions (time, rows, cols).\n",
      "\n",
      "Starting simple, let's look at the \"time\" variable."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Coordinate Variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print d.variables['time']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'netCDF4.Variable'>\n",
        "int32 time(time)\n",
        "    calendar: gregorian\n",
        "    axis: T\n",
        "    units: days since 1966-10-03\n",
        "    long_name: time\n",
        "    standard_name: time\n",
        "unlimited dimensions: time\n",
        "current shape = (1,)\n",
        "filling on, default _FillValue of -2147483647 used\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Printing an individual variable gives quite a bit more information about the variable than just printing out an overview of the file. This information tends to be directly related to the variable itself. We can see the variable is a 32 bit integer. Since it has a dimension of time, and the time dimension has a size of 1, we expect that the current_shape would be a single dimension of one value. The rest of the attributes are human friendly: you as a human can pretty much figure it out just by looking.\n",
      "\n",
      "Lets check if the contents of the time variable meet our expectations. From the filename, this file contains data for the week of Jan 7-13, 2003. Without being too careful about leap years and such, let's check to see if the time variable contains a number that works out to be about the number of days that would put us in Jan 2003 if we were to start counting on Oct 3, 1966."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print d.variables['time'][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here for the first time, we've asked for actual data from the file. To get data, you index the variable of interest. In the above, we asked for the first value in the time array (index 0). \n",
      "\n",
      "Now we just have to see if 13251 days puts us in 2003."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_years = 365.25 * (2002-1966)\n",
      "oct_to_jan = np.sum([(31-3),30,31]) # rest of oct, nov, dec\n",
      "print full_years, full_years + oct_to_jan\n",
      "print d.variables['time'][0] - (full_years + oct_to_jan)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our faith in the ability of computers to count is affirmed. All hail our cyber overlords. The day specified is Jan 13, 2003 plus or minus 1 day for alignment of leap years with our time window.\n",
      "\n",
      "The attributes \"units\", \"standard_name\", and \"axis\" are part of the CF 1.6 convention that the file respects. And since this variable is in the file adhering to CF 1.6, the variable adheres to it too. These attributes have special meanings, and you have to read the convention specification to find out what that meaning is.\n",
      "\n",
      "One more term to remember is \"coordinate variable\". A one dimensional variable that has the same name as it's dimension is a coordinate variable. Hence, the time variable is the only coordinate variable in this file. Coordinate variables are how you translate from unitless grid indices to real values along the axis. In the above, index 0 for the time dimension stands for Jan 13, 2003. This relationship holds for any variable in this file which uses the time dimension."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Auxiliary Coordinate Variables"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sometimes, the grid cells are not evenly spaced. The latitude and longitude variables for this EASE Grid 2.0 are examples of this. The data are spaced at even 100km intervals in the geospatial projection, which does not translate to nicely gridded lat/lon data. Auxiliary coordinate variables are 2D variables which can be used to lookup the latitude and longitude of any row/col pair."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print d.variables['latitude']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This variable stores the latitude of each grid cell. The fact that \"units\" is \"degrees_north\" and/or the value \"latitude\" for the standard_name attribute may be used to determine that this is an auxiliary coordinate variable, and it contains the latitude values. The name of the variable is not special. It could be called 'i_do_not_contain_latitudes', but if one or more of the special attributes contained the magic latitude-indicating values, it would still be interpreted as latitudes.\n",
      "\n",
      "Let's see what's inside."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print d.variables['latitude'][0,0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because this is a 2D variable, we had to provide a 2D index. The -- means that the latitude value at index [0,0] is nodata. The EASE Grid is a polar projection which looks more or less like a circle. The corners don't have data, so we really should have expected this. While we're here, let's load the whole latitude array into memory because it's small. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lats = d.variables['latitude'][:]\n",
      "print lats.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(180, 180)\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This gives us a new variable called \"lats\". It has a copy of the values that were in the file. All 180x180 of them. Changing the values in the \"lats\" variable does not change the values in the file itself.  We used a technique called \"slicing\" (e.g., a colon instead of an index) to load in the values we wanted. Normally, you say [min_index:max_index] if you want to specify a subset. The default value of min_index is the first array value, and the default value of max_index is the last array value. In this case, we wanted everything, so we left out both the min and the max, allowing both to take on their default values. \n",
      "\n",
      "There's actually a little bit more than this going on since we're talking about a 2D array, but that is out of scope for this notebook. Slicing is a powerful python technique which you need to spend some brain cells on. Go forth and learn.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Masked Arrays"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case, the python variable \"lats\" is what's known as a masked array. The netcdf library automatically masked out the nodata values for us, which is why we just saw a '--' above when we printed out the latitude value of the corner. Masked arrays are a combination of a boolean (True/False) mask, and the values. If the mask flag for a cell is true, the cell is considered to contain \"nodata\". So let's revisit that masked out corner."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"The mask is: \", lats.mask[0,0]\n",
      "print \"The data value is: \", lats.data[0,0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The mask is:  True\n",
        "The data value is:  -999.0\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you scroll back up, the fill value in the netCDF file was -999. So you can see that python is making things a little nicer for you in that you don't have to always check to see if the grid cell contains the fill value/nodata value.\n",
      "\n",
      "Let's set the corner value to something and then mask it back out."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lats[0,0] = 42.\n",
      "print lats[0,0]\n",
      "print \"The mask is: \", lats.mask[0,0]\n",
      "print \"The data value is: \", lats.data[0,0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "42.0\n",
        "The mask is:  False\n",
        "The data value is:  42.0\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ... and set it back\n",
      "lats[0,0] = np.ma.masked\n",
      "print lats[0,0]\n",
      "print \"The mask is: \", lats.mask[0,0]\n",
      "print \"The data value is: \", lats.data[0,0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "--\n",
        "The mask is:  True\n",
        "The data value is:  42.0\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Whoops! It masked the value out for us all right, but it didn't change the value back to -999. Keep this in mind. Depending on your situation, it might be important. You can explicitly set the value by using the \"data\" attribute. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lats.data[0,0] = -999.0\n",
      "print lats[0,0]\n",
      "print \"The mask is: \", lats.mask[0,0]\n",
      "print \"The data value is: \", lats.data[0,0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "--\n",
        "The mask is:  True\n",
        "The data value is:  -999.0\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Variables...finally"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far we have explored a whole lot of nothing useful. We know where each grid cell is, but (in this case) not whether they are covered in snow. We want to know whether they are covered in snow, so we use the non-coordinate-variable, non-auxiliary-coordinate-variable called \"merged_snow_cover_extent\". We will create a python variable as a shorthand to the data in the file, but we're not going to give a slicing expression. Without the slicing expression, python won't copy the data into memory from the file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "snow = d.variables['merged_snow_cover_extent']\n",
      "print snow"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'netCDF4.Variable'>\n",
        "int8 merged_snow_cover_extent(time, rows, cols)\n",
        "    flag_meanings: cdr_microwave_report_snow cdr_only_reports_snow microwave_only_reports_snow snow_free_land permanent_ice ocean\n",
        "    flag_values: [10 11 12 20 30 40]\n",
        "    _FillValue: -99\n",
        "    comment: 10: Snow cover reported by weekly_cdr, passive_microwave, 11: Snow cover reported by weekly_cdr only,  12: Snow cover reported by passive_microwave only, 20: Snow free land, 30: Permanent ice covered land, 40: Ocean\n",
        "    valid_range: [10 40]\n",
        "    coordinates: longitude latitude time\n",
        "    long_name: Merged Snow Cover Extent\n",
        "    grid_mapping: coord_system\n",
        "unlimited dimensions: time\n",
        "current shape = (1, 180, 180)\n",
        "filling on\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "See how the above behaved as if we typed \"print d.variables['merged_snow_cover_extent']\"? Now we can be lazy and just type \"print snow\" instead. You can still specify an index and get the data, but this time you are not working with a copy that resides in memory. Remember we did not explicitly ask python to copy all the data out of the file into a variable. Ergo, if you try and assign different values to cells in the \"snow\" variable, it will try to change the values in the file. In this case, we opened the file read-only (which is the default), so changing the value would fail."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print snow[0,0,0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "--\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "??! A 3D variable? Tricky. You can handle this by now, and you should be able to tell what order the coordinates go in by looking above.\n",
      "\n",
      "So what happens if we try to change the values in the snow variable? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#snow[0,0,0] = 42"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This translates roughly as \"Thou shalt not try to write to a file you opened as read only.\" Fear not the crappy error message. Just understand that when we created the \"snow\" variable in python, we essentially told python to treat the snow variable as a handle to the data on disk. Trying to set values in the snow variable attempts to write the new values to the file...which it can't do if the file has been opened for reading only.\n",
      "\n",
      "Now, one final thing: the coordinates attribute lists the (auxiliary) coordinate variables which contain the location information in each cell. The CF 1.6 standard requires that the lat/lon of each cell be provided if the lat/lons are not evenly spaced out. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Projection information"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you look above where we printed out the snow variable, you should see a \"grid_mapping\" attribute. This is another one of those attributes specified by CF-1.6. It contains the name of a dummy variable which describes the projection in which the data are presented. It's a dummy variable because it contains no data values, it serves only as a container for a bunch of descriptive attributes....all of which are described by CF-1.6. The snow variable declares that its projection information is contained in the \"coord_system\" dummy variable, so let's take a look. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "coords = d.variables['coord_system']\n",
      "print coords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'netCDF4.Variable'>\n",
        "|S1 coord_system()\n",
        "    comment: EASE-Grid-2.0 projection definition documention: http://nsidc.org/data/ease/ease_grid2.html, NSIDC mapx grid parameter definition file: EASE2_N100km.gpd, grid_id: EASE2_N100km\n",
        "    grid_mapping_name: lambert_azimuthal_equal_area\n",
        "    longitude_of_projection_origin: 0.0\n",
        "    latitude_of_projection_origin: 90.0\n",
        "    false_easting: 0.0\n",
        "    false_northing: 0.0\n",
        "    scale_factor_at_projection_origin: 25\n",
        "    semimajor_axis: 6.37814e+06\n",
        "    semiminor_axis: 6.35675e+06\n",
        "    inverse_flattening: 298.257\n",
        "unlimited dimensions: \n",
        "current shape = ()\n",
        "filling on, default _FillValue of \u0000 used\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The only parameter that a variable pointed to by a \"grid_mapping\" attribute must have is \"grid_mapping_name\". These names select the projection and are defined in http://cfconventions.org/Data/cf-conventions/cf-conventions-1.6/build/cf-conventions.html#appendix-grid-mappings.  The other attributes define the parameters of the projection, which of course may vary from projection to projection. The parameters of the sphereoid may appear regardless of the projection selected. \n",
      "\n",
      "This information gives enough information to calculate projected coordinates given lat/lon, or vice-versa. You will want to use pre-written library code for that."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Summary"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is what we've discovered so far: \n",
      "\n",
      "* Grid cells are regularly spaced in projected coordinates.\n",
      "* Latitude/Longitude do not comprise a regular grid.\n",
      "* There are no coordinate variables (1d) to describe rows and cols.\n",
      "* Auxiliary coordinate variables (2d) are identified in the \"coordinates\" attribute, and tie each grid cell to a set of lat/lon coordinates.\n",
      "* The names of the auxiliary coordinate variables are not special: latitude and longitude are identified by matching the \"units\" to accepted values.\n",
      "* Variables containing geolocated data refer to their spatial reference system by placing the name of a variable in the \"grid_mapping\" attribute. This variable has a series of attributes which describe the spatial reference system.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "So why isn't it working?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The big thing that is missing is a simple translation between rows/cols and locations in the projected coordinates. Presumably, once we provide this, other programs will start to grok the NetCDF file, or we can export to a different format like geotiff. Specifically, the most likely reason that the projection is not being read is that the file technically violates the CF 1.6 specification. When the projection is Lambert Azimuthal Equal Area, the spec says \"The x (abscissa) and y (ordinate) rectangular coordinates are identified by the standard_name attribute values projection_x_coordinate and projection_y_coordinate respectively.\" \n",
      "\n",
      "So, software designed to the specification is expecting coordinate variables for rows and cols, and will be looking for these variables to be tagged as \"projection_x_coordinate\" and \"projection_y_coordinate\". As noted above, these variables are entirely missing. It is possible that if they are provided, the software may start working."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Calculating projected coordinates"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are going to assume that the relationship between the grid cell indices and the projected coordinates is linear, that the axes are orthogonal, and the (i,j) grid cell axes are not rotated with respect to the axes of the projected coordinate system. We will only generalize to this more complex scenario if our assumptions turn out to be false.\n",
      "\n",
      "We need to find two points which have lat/lon data, these two points should be as far apart as possible, and the points should not be in the same column or row."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mysnow = d.variables['merged_snow_cover_extent'][:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mysnow[0,0,79:101]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "masked_array(data = [-- 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 --],\n",
        "             mask = [ True False False False False False False False False False False False\n",
        " False False False False False False False False False  True],\n",
        "       fill_value = -99)"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unmasked_ij = np.where(np.logical_not(lats.mask))\n",
      "indices = zip(*unmasked_ij)\n",
      "first = indices[0]\n",
      "last = indices[-1]\n",
      "print \"First: \", first\n",
      "print \"Last: \", last"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "First:  (0, 80)\n",
        "Last:  (179, 99)\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lons = d.variables['longitude']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"First: \", lats[first], lons[first]\n",
      "print \"Last: \", lats[last], lons[last]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "First:  0.123689 -173.941\n",
        "Last:  0.123689 6.05899\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we were trying to get a simple linear fit from rows/cols to lat/lon, this would be a problem. However, we know this is not possible so let's take these lat/lons and calculate the projected coordinates. We will be using Proj.4. (http://trac.osgeo.org/proj/)  The first step is to get the parameter values for the projection from the coord_system variable. \n",
      "\n",
      "We have not yet done this. We've printed the values and looked at them, but we are now moving into the realm of having the computer fetch specific values and use them automatically. Getting the value of an attribute in the netCDF file is accomplished exactly as if it were an attribute of a python variable: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#remember that coords is the python variable referring to the \"coord_system\" netcdf variable.\n",
      "lat_0 = coords.latitude_of_projection_origin \n",
      "lon_0 = coords.longitude_of_projection_origin\n",
      "false_e = coords.false_easting\n",
      "false_n = coords.false_northing\n",
      "s_major = coords.semimajor_axis \n",
      "s_minor = coords.semiminor_axis"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we make a projection object. It's just a handy way to carry around a description of which projection equations to use, and what the parameters are."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "the_proj = pyproj.Proj(proj=\"laea\", lat_0=lat_0, lon_0=lon_0, x_0=false_e, y_0=false_n, a=s_major, b=s_minor)\n",
      "geo      = pyproj.Proj(proj=\"latlong\", a=s_major, b=s_minor)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can compute the projected coordinates for the lat/lon coordinates of the two points."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x1,y1 = pyproj.transform(geo,the_proj, lons[first], lats[first])\n",
      "x2,y2 = pyproj.transform(geo,the_proj, lons[last], lats[last])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print x1,y1\n",
      "print x2,y2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-950000.614496 8950001.68985\n",
        "950000.167584 -8950001.73729\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Construct the linear fits"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, it's now a matter of making a linear fit for x and a linear fit for y."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# it's (rows,cols), so index[1]==column \n",
      "m_x = (x2-x1)/(last[1]-first[1])\n",
      "b_x = x1 - (m_x*first[1])\n",
      "print x2, last[1]*m_x+b_x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "950000.167584 950000.167584\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Woo hoo the fit for x seems to work. Now on to y."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# it's (rows,cols) so index[0]==rows\n",
      "m_y = (y2-y1)/(last[0]-first[0])\n",
      "b_y = y1 -(m_y*first[0])\n",
      "print y2, last[0]*m_y+b_y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-8950001.73729 -8950001.73729\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Just to doublecheck, we know that the data set is on a 100km grid, so m_x and m_y should both be 100000."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print m_x, m_y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100000.041162 -100000.019146\n"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Close enough for me."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Create the coordinate variables..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Python, specifically the \"numpy\" library, makes it easy to write equations which manipulate entire arrays of data. Usually, the equation is applied to each element of the input array and produces a corresponding output array. Numpy is well worth learning.\n",
      "\n",
      "In the following, the function \"np.arange\" generates an array containing the numbers 0, 1, 2, ... {n-1}, where n is either the number of rows or cols. This entire array is transformed with the linear fit we produced above, yielding the projected x/y coordinates for each row and column."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv_rows = m_x * np.arange(len(d.dimensions['rows'])) + b_x\n",
      "cv_cols = m_y * np.arange(len(d.dimensions['cols'])) + b_y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we need to store these new coordinate variables in the file and add the expected attributes. To do so, we need to close the netcdf file and open it again for writing. Make sure we open for read/write! Opening just for write will try to delete the existing file so you have a nice clean slate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print cv_rows\n",
      "print cv_cols\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[-8950003.90746319 -8850003.8663011  -8750003.82513901 -8650003.78397691\n",
        " -8550003.74281482 -8450003.70165273 -8350003.66049064 -8250003.61932854\n",
        " -8150003.57816645 -8050003.53700436 -7950003.49584226 -7850003.45468017\n",
        " -7750003.41351808 -7650003.37235599 -7550003.3311939  -7450003.2900318\n",
        " -7350003.24886971 -7250003.20770762 -7150003.16654552 -7050003.12538343\n",
        " -6950003.08422134 -6850003.04305925 -6750003.00189715 -6650002.96073506\n",
        " -6550002.91957297 -6450002.87841088 -6350002.83724878 -6250002.79608669\n",
        " -6150002.7549246  -6050002.71376251 -5950002.67260041 -5850002.63143832\n",
        " -5750002.59027623 -5650002.54911414 -5550002.50795204 -5450002.46678995\n",
        " -5350002.42562786 -5250002.38446577 -5150002.34330367 -5050002.30214158\n",
        " -4950002.26097949 -4850002.21981739 -4750002.1786553  -4650002.13749321\n",
        " -4550002.09633112 -4450002.05516902 -4350002.01400693 -4250001.97284484\n",
        " -4150001.93168275 -4050001.89052065 -3950001.84935856 -3850001.80819647\n",
        " -3750001.76703438 -3650001.72587228 -3550001.68471019 -3450001.6435481\n",
        " -3350001.60238601 -3250001.56122391 -3150001.52006182 -3050001.47889973\n",
        " -2950001.43773764 -2850001.39657554 -2750001.35541345 -2650001.31425136\n",
        " -2550001.27308927 -2450001.23192717 -2350001.19076508 -2250001.14960299\n",
        " -2150001.10844089 -2050001.0672788  -1950001.02611671 -1850000.98495462\n",
        " -1750000.94379252 -1650000.90263043 -1550000.86146834 -1450000.82030625\n",
        " -1350000.77914415 -1250000.73798206 -1150000.69681997 -1050000.65565788\n",
        "  -950000.61449578  -850000.57333369  -750000.5321716   -650000.49100951\n",
        "  -550000.44984741  -450000.40868532  -350000.36752323  -250000.32636114\n",
        "  -150000.28519904   -50000.24403695    49999.79712514   149999.83828723\n",
        "   249999.87944933   349999.92061142   449999.96177351   550000.00293561\n",
        "   650000.0440977    750000.08525979   850000.12642188   950000.16758398\n",
        "  1050000.20874607  1150000.24990816  1250000.29107025  1350000.33223234\n",
        "  1450000.37339444  1550000.41455653  1650000.45571862  1750000.49688072\n",
        "  1850000.53804281  1950000.5792049   2050000.62036699  2150000.66152909\n",
        "  2250000.70269118  2350000.74385327  2450000.78501536  2550000.82617746\n",
        "  2650000.86733955  2750000.90850164  2850000.94966373  2950000.99082583\n",
        "  3050001.03198792  3150001.07315001  3250001.1143121   3350001.1554742\n",
        "  3450001.19663629  3550001.23779838  3650001.27896048  3750001.32012257\n",
        "  3850001.36128466  3950001.40244675  4050001.44360884  4150001.48477094\n",
        "  4250001.52593303  4350001.56709512  4450001.60825722  4550001.64941931\n",
        "  4650001.6905814   4750001.73174349  4850001.77290559  4950001.81406768\n",
        "  5050001.85522977  5150001.89639186  5250001.93755396  5350001.97871605\n",
        "  5450002.01987814  5550002.06104023  5650002.10220233  5750002.14336442\n",
        "  5850002.18452651  5950002.2256886   6050002.2668507   6150002.30801279\n",
        "  6250002.34917488  6350002.39033698  6450002.43149907  6550002.47266116\n",
        "  6650002.51382325  6750002.55498534  6850002.59614744  6950002.63730953\n",
        "  7050002.67847162  7150002.71963372  7250002.76079581  7350002.8019579\n",
        "  7450002.84311999  7550002.88428209  7650002.92544418  7750002.96660627\n",
        "  7850003.00776836  7950003.04893046  8050003.09009255  8150003.13125464\n",
        "  8250003.17241673  8350003.21357883  8450003.25474092  8550003.29590301\n",
        "  8650003.3370651   8750003.3782272   8850003.41938929  8950003.46055138]\n",
        "[ 8950001.68985344  8850001.67070738  8750001.65156132  8650001.63241526\n",
        "  8550001.6132692   8450001.59412315  8350001.57497709  8250001.55583103\n",
        "  8150001.53668497  8050001.51753891  7950001.49839285  7850001.4792468\n",
        "  7750001.46010074  7650001.44095468  7550001.42180862  7450001.40266256\n",
        "  7350001.3835165   7250001.36437044  7150001.34522439  7050001.32607833\n",
        "  6950001.30693227  6850001.28778621  6750001.26864015  6650001.24949409\n",
        "  6550001.23034804  6450001.21120198  6350001.19205592  6250001.17290986\n",
        "  6150001.1537638   6050001.13461774  5950001.11547169  5850001.09632563\n",
        "  5750001.07717957  5650001.05803351  5550001.03888745  5450001.01974139\n",
        "  5350001.00059533  5250000.98144928  5150000.96230322  5050000.94315716\n",
        "  4950000.9240111   4850000.90486504  4750000.88571898  4650000.86657293\n",
        "  4550000.84742687  4450000.82828081  4350000.80913475  4250000.78998869\n",
        "  4150000.77084263  4050000.75169658  3950000.73255052  3850000.71340446\n",
        "  3750000.6942584   3650000.67511234  3550000.65596628  3450000.63682023\n",
        "  3350000.61767417  3250000.59852811  3150000.57938205  3050000.56023599\n",
        "  2950000.54108993  2850000.52194387  2750000.50279782  2650000.48365176\n",
        "  2550000.4645057   2450000.44535964  2350000.42621358  2250000.40706752\n",
        "  2150000.38792147  2050000.36877541  1950000.34962935  1850000.33048329\n",
        "  1750000.31133723  1650000.29219117  1550000.27304512  1450000.25389906\n",
        "  1350000.234753    1250000.21560694  1150000.19646088  1050000.17731482\n",
        "   950000.15816876   850000.13902271   750000.11987665   650000.10073059\n",
        "   550000.08158453   450000.06243847   350000.04329241   250000.02414636\n",
        "   150000.0050003     49999.98585424   -50000.03329182  -150000.05243788\n",
        "  -250000.07158394  -350000.09072999  -450000.10987605  -550000.12902211\n",
        "  -650000.14816817  -750000.16731423  -850000.18646029  -950000.20560635\n",
        " -1050000.2247524  -1150000.24389846 -1250000.26304452 -1350000.28219058\n",
        " -1450000.30133664 -1550000.3204827  -1650000.33962875 -1750000.35877481\n",
        " -1850000.37792087 -1950000.39706693 -2050000.41621299 -2150000.43535905\n",
        " -2250000.4545051  -2350000.47365116 -2450000.49279722 -2550000.51194328\n",
        " -2650000.53108934 -2750000.5502354  -2850000.56938145 -2950000.58852751\n",
        " -3050000.60767357 -3150000.62681963 -3250000.64596569 -3350000.66511175\n",
        " -3450000.68425781 -3550000.70340386 -3650000.72254992 -3750000.74169598\n",
        " -3850000.76084204 -3950000.7799881  -4050000.79913416 -4150000.81828021\n",
        " -4250000.83742627 -4350000.85657233 -4450000.87571839 -4550000.89486445\n",
        " -4650000.91401051 -4750000.93315656 -4850000.95230262 -4950000.97144868\n",
        " -5050000.99059474 -5150001.0097408  -5250001.02888686 -5350001.04803292\n",
        " -5450001.06717897 -5550001.08632503 -5650001.10547109 -5750001.12461715\n",
        " -5850001.14376321 -5950001.16290927 -6050001.18205532 -6150001.20120138\n",
        " -6250001.22034744 -6350001.2394935  -6450001.25863956 -6550001.27778562\n",
        " -6650001.29693167 -6750001.31607773 -6850001.33522379 -6950001.35436985\n",
        " -7050001.37351591 -7150001.39266197 -7250001.41180802 -7350001.43095408\n",
        " -7450001.45010014 -7550001.4692462  -7650001.48839226 -7750001.50753832\n",
        " -7850001.52668437 -7950001.54583043 -8050001.56497649 -8150001.58412255\n",
        " -8250001.60326861 -8350001.62241467 -8450001.64156073 -8550001.66070678\n",
        " -8650001.67985284 -8750001.6989989  -8850001.71814496 -8950001.73729102]\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# first, close the current file\n",
      "d.close()\n",
      "# now open again for writing\n",
      "d = nc.Dataset('nhtsw100e2_20030107_20030113.nc', 'r+')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Creating variables in netcdf is pretty simple. You just need to specify the variable name, the data type, and a list of dimensions as strings. Coordinate variables are super simple because the name of the variable is the same as the name of the only dimension they have."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "coords = d.variables['coord_system']\n",
      "print coords"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'netCDF4.Variable'>\n",
        "|S1 coord_system()\n",
        "    comment: EASE-Grid-2.0 projection definition documention: http://nsidc.org/data/ease/ease_grid2.html, NSIDC mapx grid parameter definition file: EASE2_N100km.gpd, grid_id: EASE2_N100km\n",
        "    grid_mapping_name: lambert_azimuthal_equal_area\n",
        "    longitude_of_projection_origin: 0.0\n",
        "    latitude_of_projection_origin: 90.0\n",
        "    false_easting: 0.0\n",
        "    false_northing: 0.0\n",
        "    scale_factor_at_projection_origin: 25\n",
        "    semimajor_axis: 6.37814e+06\n",
        "    semiminor_axis: 6.35675e+06\n",
        "    inverse_flattening: 298.257\n",
        "unlimited dimensions: \n",
        "current shape = ()\n",
        "filling on, default _FillValue of \u0000 used\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "coords.test = \"test\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'd' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-3-ef507f445a30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'd' is not defined"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# make the rows coordinate variable\n",
      "# note the variable name is the same as the dimension name\n",
      "rows_var = d.createVariable('rows',np.float32, ('rows',))\n",
      "\n",
      "# give it the expected attributes\n",
      "rows_var.standard_name = 'projection_y_coordinate'\n",
      "rows_var.axis          = 'Y'\n",
      "rows_var.units         = 'meters'\n",
      "\n",
      "# write the values to the variable\n",
      "rows_var[:] = cv_rows"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# make the cols coordinate variable\n",
      "# note the variable name is the same as the dimension name\n",
      "cols_var = d.createVariable('cols',np.float32, ('cols',))\n",
      "\n",
      "# give it the expected attributes\n",
      "cols_var.standard_name = 'projection_x_coordinate'\n",
      "cols_var.axis          = 'X'\n",
      "cols_var.units         = 'meters'\n",
      "\n",
      "# write the values to the variable\n",
      "cols_var[:] = cv_cols"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#close the file to make sure everything is written\n",
      "d.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Poof. GDAL works now: \n",
      "\n",
      "    [fedora@lugosi ~]$ gdalinfo NETCDF:nhtsw100e2_20030107_20030113.nc:merged_snow_cover_extent\n",
      "    Driver: netCDF/Network Common Data Format\n",
      "    Files: nhtsw100e2_20030107_20030113.nc\n",
      "    Size is 180, 180\n",
      "    Coordinate System is:\n",
      "    PROJCS[\"LAEA (WGS84) \",\n",
      "        GEOGCS[\"WGS 84\",\n",
      "            DATUM[\"WGS_1984\",\n",
      "                SPHEROID[\"WGS 84\",6378137,298.257223563,\n",
      "                    AUTHORITY[\"EPSG\",\"7030\"]],\n",
      "                TOWGS84[0,0,0,0,0,0,0],\n",
      "                AUTHORITY[\"EPSG\",\"6326\"]],\n",
      "            PRIMEM[\"Greenwich\",0,\n",
      "                AUTHORITY[\"EPSG\",\"8901\"]],\n",
      "            UNIT[\"degree\",0.0174532925199433,\n",
      "                AUTHORITY[\"EPSG\",\"9108\"]],\n",
      "            AUTHORITY[\"EPSG\",\"4326\"]],\n",
      "        PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],\n",
      "        PARAMETER[\"latitude_of_center\",90],\n",
      "        PARAMETER[\"longitude_of_center\",0],\n",
      "        PARAMETER[\"false_easting\",0],\n",
      "        PARAMETER[\"false_northing\",0]]\n",
      "        \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Geotiffs via gdal_translate"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because gdal is now detecting the spatial reference system, one can use it to convert the variables to geotiff. But we are not quite done. Remember the slope of the line fit between \"cols\" and the projected y coordinate? It was negative, right? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print m_y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Yup. Negative. That means row zero translates to the biggest projected Y value and row 179 translates to the smallest projected Y value. So row 0 starts at the top and works its way down. The netcdf driver for gdal tries to detect whether the rows are top-down or bottom up, but sometimes it doesn't. Our data set is one of the lucky winners.\n",
      "\n",
      "Configuration options from http://www.gdal.org/frmt_netcdf.html:  \n",
      "\n",
      "> **GDAL_NETCDF_BOTTOMUP**=[YES/NO] : Set the y-axis order for import, overriding the order detected by the driver. This option is usually not needed unless a specific dataset is causing problems (which should be reported in GDAL trac).\n",
      "\n",
      "In this case, gdal 1.11.1 autodetects a bottom-up dataset when it should be top down, so we must override. The working conversion command for gdal_translate is as follows: \n",
      "     \n",
      "    gdal_translate -of GTiff -b 1 --config GDAL_NETCDF_BOTTOMUP NO \\\n",
      "        NETCDF:nhtsw100e2_20030107_20030113:merged_snow_cover_extent snowcover.tif"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Looking at the hard won data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we view the data in the file. First, we don't do anything special, just dump it to the screen exactly as it exists in the file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = nc.Dataset('nhtsw100e2_20030107_20030113.nc')\n",
      "snow = d.variables['merged_snow_cover_extent'][:]\n",
      "lats = d.variables['latitude'][:]\n",
      "lons = d.variables['longitude'][:]\n",
      "imshow(snow[0,...])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"hello\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That's great, but we don't really have any indication that the georeferencing worked. The axes are strictly (i,j) without any physical meaning. In fact, we could have done this right off the bat, without fixing anything. What we really want to do is draw a quick map with crude coastlines and some lat/lon lines."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def init_basemap() : \n",
      "\n",
      "    m = Basemap(width=100000*180, height=100000*180,\n",
      "            #llcrnrx=cv_cols[0], llcrnry=cv_rows[-1], urcrnrx=cv_cols[-1], urcrnry=cv_rows[0],\n",
      "            resolution='l', projection='laea',\n",
      "            lon_0=lon_0, lat_0=lat_0)\n",
      "    m.drawcoastlines()\n",
      "    m.drawcountries()\n",
      "    m.drawmeridians(np.arange(-180.,180.,20.),labels=[False,False,False,True])\n",
      "    m.drawparallels(np.arange(10.,80.,20.), labels=[True,False,False,False])\n",
      "    return m"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The basic strategy is to use the map template above by calling \"init_basemap\", then draw the data on it. The data are drawn using \"filled contours\" between the data points. This may or may not be what we want."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "figsize(12,10)\n",
      "m = init_basemap()\n",
      "c = m.contourf(lons[:], lats[:],\n",
      "               snow[0,...], 20, latlon=True)\n",
      "cb = m.colorbar(c)\n",
      "title(\"Snow cover, 13 Jan 2003\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As shown, the data values line up with the coastlines, so it appears that the georeferencing worked just fine. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}